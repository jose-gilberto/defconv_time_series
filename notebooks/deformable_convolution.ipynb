{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\mathcal{R}$ be a $3 \\times 3$ kernel used to sample a small region of the input.\n",
    "\n",
    "$$\n",
    "\\mathcal{R} = \\{ (-1, -1), (-1, 0), \\cdots, (0, 1), (1, 1) \\}\n",
    "$$\n",
    "\n",
    "Then the equation of the normal 2d convolution operation will be given as shown in the figure below where $w$ is the weights of the kernel, $x$ is the input feature map, $y$ is the output of convolution operation, $p_0$ is the starting position of each kernel and $p_n$ is enumerating along with all the positions in $\\mathcal{R}$.\n",
    "\n",
    "$$\n",
    "y(p_0) = \\sum_{p_n \\in \\mathcal{R}} w(p_n) \\cdot x(p_0 + p_n)\n",
    "$$\n",
    "\n",
    "The equation denotes the convolution operation where each position on the sampled frid is first multiplied by the corresponding value of the weight matrix and then summed to give a scalar output and repeating the same operation over the entire image give us the new feature map.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deformable convolution instead of using a simple fixed sampling grid introduces 2D offsets to the normal convolution operation depicted above.\n",
    "\n",
    "If $\\mathcal{R}$ is the normal grid, then the deformable convolution operation augments learned offsets to the grid, thereby deforming the sampling position of the grid.\n",
    "\n",
    "The deformable convolution operation is depicted by the equation below where $\\Delta p_n$ denotes the offets added to the normal convolution.\n",
    "\n",
    "$$\n",
    "y(p_0) = \\sum_{p_n \\in \\mathcal{R}} w(p_n) \\cdot x(p_0 + p_n + \\Delta p_n)\n",
    "$$\n",
    "\n",
    "Now as the sampling is done on the irregular and offset locations and $\\Delta p_n$ is generally fractional, we use bilinear interpolation to implement the above equation. \n",
    "\n",
    "**Bilinear interpolation** is used because as we add offsets to the existing sampling positions, we obtain fractional points which are not defined locations on the grid and i order to estimate their values we use bilinear interpolation which uses a 2x2 grid of the neighbouring values to estimate the value of the new deformed position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eq. that is used to perform bi-linear interpolation and estimate the pixel value at the fractional position is given below where $p(p_0 + p_n + \\Delta p_n)$ is the deformed position, $q$ enumerates all the valid positions on the input feature map and $G(.)$ is the bilinear interpolation kernel.\n",
    "\n",
    "$$\n",
    "x(p) = \\sum_q G(q, p) \\cdot x(q)\n",
    "$$\n",
    "\n",
    "Note: G(..) is a 2 dimensional and can be broken down according to the axis into two one dimensional kernel as shown below\n",
    "\n",
    "$$\n",
    "G(q, p) = g(q_x, p_x) \\cdot g(q_y, p_y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Visual Representation of Deformable Convolution](deform_convolution.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in Figure above, the offsets are obtained by applying a convolution layer over the input feature map. The convolution kernel used has spatial resolution and dilation as those of the current convolution layer. The output offset field has the same resolution as that of the input feature map and has $2N$ channels where $2N$ correspond to $N$ 2d offsets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As illustrated above, the offsets are obtained by applying a convolutional layer over the same input feature map. The convolution kernel is of the same spatial resolution and dilation as those of the current convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(4, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_channels = 4\n",
    "kernel_size = 3\n",
    "stride = 1\n",
    "padding = 1\n",
    "\n",
    "lr_ratio = 1.\n",
    "\n",
    "offset_conv = nn.Conv2d(\n",
    "    in_channels=in_channels,\n",
    "    out_channels=2 * kernel_size * kernel_size, # 2 * 3 * 3 => (2N) => N is the number of values in the kernel\n",
    "    kernel_size=kernel_size, # The same spatial resolution and dilation as those of the curent convolutional layer\n",
    "    stride=stride,\n",
    "    padding=padding\n",
    ")\n",
    "\n",
    "offset_conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the training, these added conv and fc layers for offset learning are initialized with zero weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7f6adb01a910>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This function allows a custom learning rate for the offset layers\n",
    "def _set_lr(module, grad_input, grad_output):\n",
    "    new_grad_input = []\n",
    "\n",
    "    for i in range(len(grad_input)):\n",
    "        if grad_input[i] is not None:\n",
    "            new_grad_input.append(grad_input[i] * lr_ratio)\n",
    "        else:\n",
    "            new_grad_input.append(grad_input[i])\n",
    "    new_grad_input = tuple(new_grad_input)\n",
    "\n",
    "    return new_grad_input\n",
    "\n",
    "nn.init.constant_(offset_conv.weight, 0)  # the offset learning are initialized with zero weights\n",
    "\n",
    "offset_conv.register_backward_hook(_set_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step as illustred is to apply a offset conv into the sample. Initially this offset will have only the bias term since we setup all weights to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gbarbosa/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 18, 100, 100])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = torch.randn((1, 4, 100, 100)) # Creating a 1 sample with 4 channels with 100x100 as size\n",
    "\n",
    "offset = offset_conv(sample)\n",
    "offset.shape # As a result we have a output with shape equals to 1, 2N, 100, 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The offset obtained here are equivalent to $\\Delta p_n$ in the equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('torch.FloatTensor', 3, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtype = offset.data.type()\n",
    "ks = kernel_size\n",
    "N = offset.size(1) // 2 # Number of elements that are in the kernel (3x3 kernel = 9 elements)\n",
    "\n",
    "dtype, ks, N "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to obtain $p$ value, we must calculate $p_0$ and $p_n$ since we already have $\\Delta p_n$ values. First lets start with $p_n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 18, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gbarbosa/anaconda3/lib/python3.11/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "h, w = offset.size(2), offset.size(3) # Obtaining the height and the width of the input\n",
    "\n",
    "# (1, 2N, 1, 1)\n",
    "# TODO: only had to understand why apply a meshgrid here.\n",
    "p_n_x, p_n_y = torch.meshgrid(\n",
    "    torch.arange(-(kernel_size - 1) // 2, (kernel_size - 1) // 2 + 1),\n",
    "    torch.arange(-(kernel_size - 1) // 2, (kernel_size - 1) // 2 + 1)\n",
    ")\n",
    "\n",
    "# print(p_n_x)\n",
    "# print(p_n_y)\n",
    "\n",
    "# (2N, 1)\n",
    "p_n = torch.cat([torch.flatten(p_n_x), torch.flatten(p_n_y)], 0)\n",
    "\n",
    "p_n = p_n.view(1, 2 * N, 1, 1).type(dtype)\n",
    "\n",
    "\n",
    "p_n.requires_grad = False\n",
    "\n",
    "print(p_n.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So lets go to the $p_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 18, 100, 100])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_0_x, p_0_y = torch.meshgrid(\n",
    "    torch.arange(1, h * stride + 1, stride),\n",
    "    torch.arange(1, w * stride + 1, stride)\n",
    ")\n",
    "\n",
    "p_0_x = torch.flatten(p_0_x).view(1, 1, h, w).repeat(1, N, 1, 1)\n",
    "p_0_y = torch.flatten(p_0_y).view(1, 1, h, w).repeat(1, N, 1, 1)\n",
    "\n",
    "p_0 = torch.cat([p_0_x, p_0_y], 1).type(dtype)\n",
    "\n",
    "p_0.requires_grad = False\n",
    "\n",
    "p_0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we calculate $p = p_0 + p_n + \\Delta p_n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 18, 100, 100])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = p_0 + p_n + offset\n",
    "p.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only do a permutation to make it more easier to do the interpolation between the scalar values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 100, 18])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = p.contiguous().permute(0, 2, 3, 1)\n",
    "p.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $q$ is a float value, using bilinear interpolation, it has four integer values in the grid corresponding to that position (cause we are in a image). The four positions is left top, right top, left bottom, and right bottom, defined as: $q_{lt}, q_{rb}, q_{lb}, q_{rt}$. To calculate $q_{lt}$ we simply can take the floor of the value $q$.\n",
    "\n",
    "```\n",
    "(y,   x)   (y+1,   x)\n",
    "(y, x+1)   (y+1, x+1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 100, 18])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_lt = p.detach().floor()\n",
    "q_lt.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to have $q_{lt}$, the next value is $q_{rb}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 100, 18])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_rb = q_lt + 1\n",
    "q_rb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 100, 18])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since its 2N the first N parts are the first coordinates and so on so long\n",
    "# We ensure that q_lt are in a valid interval 0 <= p_y < h - 1\n",
    "# and 0 <= p_x <= w - 1.\n",
    "\n",
    "q_lt = torch.cat([\n",
    "    torch.clamp(q_lt[..., :N], 0, sample.size(2) - 1),\n",
    "    torch.clamp(q_lt[..., N:], 0, sample.size(3) - 1)], dim=-1).long()\n",
    "\n",
    "q_lt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 100, 18])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same thing here\n",
    "\n",
    "q_rb = torch.cat([\n",
    "    torch.clamp(q_rb[..., :N], 0, sample.size(2) - 1),\n",
    "    torch.clamp(q_rb[..., N:], 0, sample.size(3) - 1)], dim=-1).long()\n",
    "\n",
    "q_rb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100, 100, 18])\n",
      "torch.Size([1, 100, 100, 18])\n"
     ]
    }
   ],
   "source": [
    "# For $q_{lb}$ its x is equal to right bottom, its y is equal to left top.\n",
    "# Therefore, its y is from q_lt, its x is from q_rb\n",
    "\n",
    "q_lb = torch.cat([q_lt[..., :N], q_rb[..., N:]], -1)\n",
    "print(q_lb.shape)\n",
    "\n",
    "# $y$ from $q_{rb}$ and x from $q_{lt}$\n",
    "# For right top point, its $x$ is equal t to left top, its $y$ is equal to right bottom.\n",
    "\n",
    "q_rt = torch.cat([q_rb[..., :N], q_lt[..., N:]], -1)\n",
    "print(q_rt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 100, 18])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "find p_y <= padding or p_y >= h - 1 - padding, find p_x <= padding or p_x >= x - 1 - padding\n",
    "This is to find the points in the area where the pixel value is meaningful.\n",
    "\"\"\"\n",
    "# (b, h, w, N)\n",
    "mask = torch.cat([\n",
    "    p[..., :N].lt(padding) + p[..., :N].gt(sample.size(2) - 1 - padding),\n",
    "    p[..., N:].lt(padding) + p[..., N:].gt(sample.size(3) - 1 - padding)], dim=-1).type_as(p)\n",
    "\n",
    "mask = mask.detach()\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 100, 18])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "floor_p = torch.floor(p)\n",
    "floor_p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "when mask is 1, take floor_p;\n",
    "when mask is 0, take original p.\n",
    "When thr point in the padding area, interpolation is not meaningful and we can take the nearest\n",
    "point which is the most possible to have meaningful value.\n",
    "\"\"\"\n",
    "p = p * (1 - mask) + floor_p * mask\n",
    "p = torch.cat([\n",
    "    torch.clamp(p[..., :N], 0, sample.size(2) - 1),\n",
    "    torch.clamp(p[..., N:], 0, sample.size(3) - 1)], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we must apply the bilinear interpolation to find each valid value in the original grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bilinear kernel (b, h, w, N)\n",
    "g_lt = (1 + (q_lt[..., :N].type_as(p) - p[..., :N])) * (1 + (q_lt[..., N:].type_as(p) - p[..., N:]))\n",
    "g_rb = (1 - (q_rb[..., :N].type_as(p) - p[..., :N])) * (1 - (q_rb[..., N:].type_as(p) - p[..., N:]))\n",
    "g_lb = (1 + (q_lb[..., :N].type_as(p) - p[..., :N])) * (1 - (q_lb[..., N:].type_as(p) - p[..., N:]))\n",
    "g_rt = (1 - (q_rt[..., :N].type_as(p) - p[..., :N])) * (1 + (q_rt[..., N:].type_as(p) - p[..., N:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 100, 18])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_lt[:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 100, 9])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_lt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 1.0101, 0.0000, 1.0119, 2.0175, 1.8959, 2.0392,\n",
       "        0.0000, 1.1488, 1.8343, 0.0000, 0.0000, 2.1384, 0.0000, 1.0210, 1.9317],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[0,0,0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the paper:\n",
    "$$\n",
    "G(q, p) = g(q_x, p_x) \\cdot g(q_y, p_y)\n",
    "$$\n",
    "\n",
    "$$\n",
    "g(a, b) = max(0, 1-|a-b|)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_x_q(x, q, N):\n",
    "        b, h, w, _ = q.size()\n",
    "        padded_w = x.size(3)\n",
    "        \n",
    "        c = x.size(1)\n",
    "        \n",
    "        # (b, c, h*w)\n",
    "        x = x.contiguous().view(b, c, -1)\n",
    "\n",
    "        # (b, h, w, N)\n",
    "        index = q[..., :N] * padded_w + q[..., N:]  # offset_x*w + offset_y\n",
    "        # (b, c, h*w*N)\n",
    "        index = index.contiguous().unsqueeze(dim=1).expand(-1, c, -1, -1, -1).contiguous().view(b, c, -1)\n",
    "\n",
    "        x_offset = x.gather(dim=-1, index=index).contiguous().view(b, c, h, w, N)\n",
    "\n",
    "        return x_offset\n",
    "\n",
    "\n",
    "# (b, c, h, w, N)\n",
    "x_q_lt = _get_x_q(sample, q_lt, N)\n",
    "x_q_rb = _get_x_q(sample, q_rb, N)\n",
    "x_q_lb = _get_x_q(sample, q_lb, N)\n",
    "x_q_rt = _get_x_q(sample, q_rt, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 100, 100, 9])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_q_lt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 100, 100, 9])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    In the paper, x(p) = Î£G(p, q) * x(q), G is bilinear kernal\n",
    "\"\"\"\n",
    "# (b, c, h, w, N)\n",
    "x_offset = g_lt.unsqueeze(dim=1) * x_q_lt + \\\n",
    "    g_rb.unsqueeze(dim=1) * x_q_rb + \\\n",
    "    g_lb.unsqueeze(dim=1) * x_q_lb + \\\n",
    "    g_rt.unsqueeze(dim=1) * x_q_rt\n",
    "x_offset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reshape_x_offset(x_offset, ks):\n",
    "    b, c, h, w, N = x_offset.size()\n",
    "    x_offset = torch.cat([x_offset[..., s:s + ks].contiguous().view(b, c, h, w * ks) for s in range(0, N, ks)],\n",
    "                            dim=-1)\n",
    "    x_offset = x_offset.contiguous().view(b, c, h * ks, w * ks)\n",
    "\n",
    "    return x_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 4, 100, 100, 9]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/gbarbosa/Projects/defconv-ts/notebooks/deformable_convolution.ipynb Cell 40\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gbarbosa/Projects/defconv-ts/notebooks/deformable_convolution.ipynb#X54sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m conv \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mConv2d(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gbarbosa/Projects/defconv-ts/notebooks/deformable_convolution.ipynb#X54sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     in_channels\u001b[39m=\u001b[39min_channels,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gbarbosa/Projects/defconv-ts/notebooks/deformable_convolution.ipynb#X54sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     out_channels\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gbarbosa/Projects/defconv-ts/notebooks/deformable_convolution.ipynb#X54sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     bias\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gbarbosa/Projects/defconv-ts/notebooks/deformable_convolution.ipynb#X54sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gbarbosa/Projects/defconv-ts/notebooks/deformable_convolution.ipynb#X54sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# x_offset = _reshape_x_offset(x_offset, ks)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/gbarbosa/Projects/defconv-ts/notebooks/deformable_convolution.ipynb#X54sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m out \u001b[39m=\u001b[39m conv(x_offset)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gbarbosa/Projects/defconv-ts/notebooks/deformable_convolution.ipynb#X54sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m out\u001b[39m.\u001b[39mshape\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_conv_forward(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(\u001b[39minput\u001b[39m, weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    457\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 4, 100, 100, 9]"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "x_offset is kernel_size * kernel_size(N) times x. \n",
    "\"\"\"\n",
    "conv = nn.Conv2d(\n",
    "    in_channels=in_channels,\n",
    "    out_channels=4,\n",
    "    kernel_size=kernel_size,\n",
    "    stride=kernel_size,\n",
    "    bias=None\n",
    ")\n",
    "\n",
    "# x_offset = _reshape_x_offset(x_offset, ks)\n",
    "\n",
    "out = conv(x_offset)\n",
    "out.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
