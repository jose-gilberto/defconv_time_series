{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.modules.utils import _single, _reverse_repeat_tuple\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from typing import Tuple, Union, Literal, Callable, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deformable Convolutions applied to Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiments we define a kernel used in a convolution as the following way. Let $\\mathcal{R}$ be a used kernel of size 3 to sample a small region of the input. We can define this kernel as the positions:\n",
    "\n",
    "$$\n",
    "\\mathcal{R} = \\{ (-1), (0), (+1) \\}\n",
    "$$\n",
    "\n",
    "Note that for each position $i$ in the kernel, $\\mathcal{R}_i$ represents a index value sparsed in a simetric way, this will make more easier for us to apply the offsets. Also, for each $\\mathcal{R}_i$ we have an weight $k_{\\mathcal{R}_i}$ associated to that position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, for that kernel with size $3$, we have a tensor storing all the weights for each position.\n",
    "\n",
    "```python\n",
    "k = torch.Tensor([0.32, 0.21, -0.34])\n",
    "```\n",
    "\n",
    "Where $k$ is the kernel weights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formulating a basic one-dimensional convolution operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the simplest case, the output of a one-dimensional convolutional layer with input $x$ of size $(N, C_{in}, L)$ and output $y$ of size $(N, C_{out}, L)$ can be precisely described as:\n",
    "\n",
    "$$\n",
    "y(N_i, C_{out_j}, p_0) = \\text{bias}(C_{out_j}) + \\sum^{C_{in} - 1}_{k=0} \\sum_{p_n \\in \\mathcal{R}} w(C_{out_j}, k, p_n) ~ \\cdot ~ {x}(N_i, k, p_0 + p_n)\n",
    "$$\n",
    "\n",
    "where $N$ is a batch size, $C$ denotes a number of channels, $L$ is  length of signal sequence, $p_0$ is the starting position of each kernel and $p_n$ is enumerating along with all the positions in $\\mathcal{R}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different for the normal convolution, the deformable convolution instead of using just a simple fixed sampling grid, introduces offsets to the normal convolution operation. If $\\mathcal{R}$ is the normal grid, then the deformable convolution operation augments learned offsets to the grid, thereby deforming the sampling position of the grid.\n",
    "\n",
    "This operation can be explained by the following equation:\n",
    "\n",
    "$$\n",
    "y(N_i, C_{out_j}, p_0) = \\text{bias}(C_{out_j}) + \\sum^{C_{in} - 1}_{k=0} \\sum_{p_n \\in \\mathcal{R}} w(C_{out_j}, k, p_n) \\cdot x(N_i, k, p_0 + p_n + \\Delta p_n)\n",
    "$$\n",
    "\n",
    "where the new term $\\Delta p_n$ denotes the offsets added to the normal convolution.\n",
    "\n",
    "**Note 1**: As the sampling is done on the irregular and offset location, and $\\Delta p_n$ is generally fractional, we use linear interpolation to implement the above equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear Interpolation\n",
    "\n",
    "We use Linear Interpolation because as we add offsets to the existing sampling positions, we obtain fractional points, which are not defined locations on the grid. In order to estimate their values we use linear interpolation which uses 2 of the neighbouring values to estimate the value of the new deformed position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation that is used to perform a linear interpolation and estimate the pixel value a the fractional position is given below where $p = p_0 + p_n + \\Delta p_n$ is the deformed position, $q$ enumerates all the valid positions on the input feature map $x$ and $G(\\cdot)$ is the linear interpolation kernel.\n",
    "\n",
    "$$\n",
    "x(p) = \\sum_q G(q, p) \\cdot x(q)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: formulate the new equation for one dimensional linear interpolation instead of the bilinear used for 2D images.\n",
    "\n",
    "Note that $G$ is two dimensional and it is separated into two one dimensional kernels.\n",
    "\n",
    "$$\n",
    "G(q, p) = g(q_x, p_x) \\cdot g(q_y, p_y)\n",
    "$$\n",
    "\n",
    "where $g(a,b) = \\max(0, 1 - |a-b|)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modulated Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y(p) = \\sum^{K}_{k=1} w_k \\cdot x(p + p_k + \\Delta p_k) \\cdot \\Delta m_k\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_interpolation(\n",
    "    x: torch.Tensor,\n",
    "    offsets: torch.Tensor,\n",
    "    kernel_size: int,\n",
    "    dilation: int,\n",
    "    stride: int,\n",
    "    dilated_positions = None,\n",
    "    device: str = 'cpu',\n",
    "    unconstrained: bool = False\n",
    ") -> None:\n",
    "    # Ensure that the x and offsets are in the same device\n",
    "    assert x.device == offsets.device, 'The tensors x and offsets must be on same device.'\n",
    "    \n",
    "    # Calculate the receptive field for that kernel\n",
    "    kernel_rfield = dilation * (kernel_size - 1) + 1\n",
    "    \n",
    "    # Every index in x (input) we need to consider\n",
    "    if dilated_positions == None:\n",
    "        dilated_positions = torch.linspace(\n",
    "            0,\n",
    "            kernel_rfield - 1,\n",
    "            kernel_size,\n",
    "            device=offsets.device,\n",
    "            dtype=offsets.dtype\n",
    "        )\n",
    "        \n",
    "    max_t0 = (offsets.shape[-2] - 1) * stride\n",
    "    t0s = torch.linspace(0, max_t0, offsets.shape[-2], device=offsets.device, dtype=offsets.dtype).unsqueeze(-1)\n",
    "    dilated_offsets_repeated = dilated_positions + offsets\n",
    "    \n",
    "    T = t0s + dilated_offsets_repeated # batch_size x channels x out_length x kernel_size\n",
    "    if not unconstrained:\n",
    "        T = torch.max(T, t0s)\n",
    "        T = torch.min(T, t0s + torch.max(dilated_positions))\n",
    "    else:\n",
    "        T = torch.clamp(T, 0.0, float(x.shape[-1]))\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        U = torch.floor(T).to(torch.long)\n",
    "        U = torch.clamp(U, min=0, max=x.shape[-2] - 2)\n",
    "        \n",
    "        U = torch.stack([U, U + 1], dim=-1)\n",
    "\n",
    "        if U.shape[1] < x.shape[1]:\n",
    "            U = U.repeat(1, x.shape[1], 1, 1, 1)\n",
    "    \n",
    "    x = x.unsqueeze(-1).repeat(1, 1, 1, U.shape[-1])\n",
    "\n",
    "    x = torch.stack([\n",
    "        x.gather(index=torch.clamp(U[:, :, :, i, :], 0, x.shape[-2] - 1), dim=-2)\n",
    "        for i in range(U.shape[-2])], dim=-1)\n",
    "    \n",
    "    G = torch.max(\n",
    "        torch.zeros(U.shape, device=device),\n",
    "        1 - torch.abs(U - T.unsqueeze(-1))\n",
    "    )\n",
    "    \n",
    "    mx = torch.multiply(G, x.moveaxis(-2, -1))\n",
    "    return torch.sum(mx, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General and Basic Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Figure below shows an example of the overall strategy used to implement the deformable convolution.\n",
    "\n",
    "<div align='center'>\n",
    "    <img src='./deformable_convolution.png'>\n",
    "</div>\n",
    "\n",
    "As shown in this Figure, the offsets are obtained by applying a convolution layer over the input. The convolution kernel used has spatial resolution and dilation as those of the current convolution layer. The output offset has the same resolution as that of the input and has $C_{\\text{off}}$ channels, where $C_{\\text{off}}$ in that case correspond to $N$ 1d offsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeformableConvolution1d(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 kernel_size: int,\n",
    "                 stride: int = 1,\n",
    "                 padding: Union[int, Literal['valid', 'same']] = 'valid',\n",
    "                 dilation: int = 1,\n",
    "                 groups: int = 1,\n",
    "                 bias: bool = True,\n",
    "                 padding_mode: str = 'reflect',\n",
    "                 device: str = 'cpu',\n",
    "                 interpolation_func: Callable = linear_interpolation,\n",
    "                 unconstrained: str = None,\n",
    "                 *args,\n",
    "                 **kwargs) -> None:\n",
    "        \n",
    "        self.device = device\n",
    "        self.interpolation_func = interpolation_func\n",
    "        padding_ = padding if isinstance(padding, str) else _single(padding)\n",
    "        stride_ = _single(stride)\n",
    "        dilation_ = _single(dilation)\n",
    "        kernel_size_ = _single(kernel_size)\n",
    "        \n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        if groups < 0:\n",
    "            raise ValueError('groups must be a positive integer')\n",
    "        if in_channels % groups != 0:\n",
    "            raise ValueError('input channels must be divisible by groups')\n",
    "        if out_channels % groups != 0:\n",
    "            raise ValueError('out channels must be divisible by groups')\n",
    "        \n",
    "        valid_padding_strings = {'same', 'valid'}\n",
    "        if isinstance(padding, str):\n",
    "            if padding not in valid_padding_strings:\n",
    "                raise ValueError('invalid padding string, you must use valid or same')\n",
    "            if padding == 'same' and any(s != 1 for s in stride_):\n",
    "                raise ValueError('padding=same is not supported for strided convolutions')\n",
    "            \n",
    "        valid_padding_modes = {'zeros', 'reflect', 'replicate', 'circular'}\n",
    "        if padding_mode not in valid_padding_modes:\n",
    "            raise ValueError('invalid padding mode, you must use zeros, reflect, replicate or circular')\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding_\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "        self.padding_mode = padding_mode\n",
    "        \n",
    "        if isinstance(self.padding, str):\n",
    "            self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size_)\n",
    "            if padding == 'same':\n",
    "                for d, k, i in zip(dilation_, kernel_size_, range(len(kernel_size_) - 1, -1, -1)):\n",
    "                    total_padding = d * (k - 1)\n",
    "                    left_pad = total_padding // 2\n",
    "                    self._reversed_padding_repeated_twice[2 * i] = left_pad\n",
    "                    self._reversed_padding_repeated_twice[2 * i + 1] = (\n",
    "                        total_padding - left_pad\n",
    "                    )\n",
    "        else:\n",
    "            self._reversed_padding_repeated_twice = _reverse_repeat_tuple(self.padding, 2)\n",
    "            \n",
    "        self.weight = Parameter(\n",
    "            torch.empty(out_channels, in_channels // groups, kernel_size)\n",
    "        )\n",
    "        \n",
    "        self.dilated_positions = torch.linspace(\n",
    "            0, dilation * kernel_size - dilation, kernel_size\n",
    "        )\n",
    "        \n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.empty(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "            \n",
    "        if not unconstrained == None:\n",
    "            self.unconstrained = unconstrained\n",
    "            \n",
    "        self.reset_parameters()\n",
    "        self.to(device)\n",
    "        \n",
    "    def reset_parameters(self) -> None:\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        \n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "            \n",
    "    def extra_repr(self) -> str:\n",
    "        s = ('{in_channels}, {out_channels}, kernel_size={kernel_size}'\n",
    "             ', stride={stride}')\n",
    "        if self.padding != (0,) * len(self.padding):\n",
    "            s += ', padding = {padding}'\n",
    "        # if self.dilation != (1,) * len(self.dilation):\n",
    "        s += ', dilation={dilation}'\n",
    "        # if self.output_padding != (0,) * len(self.output_padding):\n",
    "        #     s += ', output_padding={output_padding}'\n",
    "        if self.groups != 1:\n",
    "            s += ', groups={groups}'\n",
    "        if self.bias is None:\n",
    "            s += ', bias=False'\n",
    "        if self.padding_mode != 'zeros':\n",
    "            s += ', padding_mode={padding_mode}'\n",
    "\n",
    "        return s.format(**self.__dict__)\n",
    "    \n",
    "    def __setstate__(self, state) -> None:\n",
    "        super().__setstate__(state)\n",
    "        if not hasattr(self, 'padding_mode'):\n",
    "            self.padding_mode = 'zeros'\n",
    "            \n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                offsets: torch.Tensor,\n",
    "                mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        in_shape = x.shape\n",
    "        if self.padding_mode != 'zeros':\n",
    "            x = F.pad(\n",
    "                x,\n",
    "                self._reversed_padding_repeated_twice,\n",
    "                mode=self.padding_mode\n",
    "            )\n",
    "        elif self.padding == 'same':\n",
    "            x = F.pad(\n",
    "                x,\n",
    "                self._reversed_padding_repeated_twice,\n",
    "                mode='constant',\n",
    "                value=0\n",
    "            )\n",
    "            \n",
    "        if not self.device == offsets.device:\n",
    "            self.device = offsets.device\n",
    "        if self.dilated_positions.device != self.device:\n",
    "            self.dilated_positions = self.dilated_positions.to(self.device)\n",
    "            \n",
    "        if 'unconstrained' in self.__dict__.keys():\n",
    "            x = self.interpolation_func(\n",
    "                x,\n",
    "                kernel_size=self.kernel_size,\n",
    "                dilation=self.dilation,\n",
    "                offsets=offsets,\n",
    "                stride=self.stride,\n",
    "                dilated_positions=self.dilated_positions,\n",
    "                device=self.device,\n",
    "                unconstrained=self.unconstrained\n",
    "            )\n",
    "        else:\n",
    "            x = self.interpolation_func(\n",
    "                x,\n",
    "                kernel_size=self.kernel_size,\n",
    "                dilation=self.dilation,\n",
    "                offsets=offsets,\n",
    "                stride=self.stride,\n",
    "                dilated_positions=self.dilated_positions,\n",
    "                device=self.device\n",
    "            )\n",
    "            \n",
    "        x = x.flatten(-2, -1)\n",
    "        output = F.conv1d(\n",
    "            x,\n",
    "            weight=self.weight,\n",
    "            bias=self.bias,\n",
    "            stride=self.kernel_size,\n",
    "            groups=self.groups\n",
    "        )\n",
    "        \n",
    "        if self.padding == 'same':\n",
    "            assert in_shape[-1] == output.shape[-1], f'input length {in_shape} and output length {output.shape} do not match'\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-9\n",
    "\n",
    "class GlobalLayerNormalization(nn.Module):\n",
    "    def __init__(self, channel_size) -> None:\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.Tensor(1, 1, channel_size))\n",
    "        self.beta = nn.Parameter(torch.Tensor(1, 1, channel_size))\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self) -> None:\n",
    "        self.gamma.data.fill_(1)\n",
    "        self.beta.data.zero_()\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        mean = x.mean(dim=1, keepdim=True).mean(dim=2, keepdim=True)\n",
    "        var = (\n",
    "            (torch.pow(x - mean, 2)).mean(dim=1, keepdim=True).mean(dim=2, keepdim=True)\n",
    "        )\n",
    "        gln_x = self.gamma * (x - mean) / torch.pow(var + EPS, 0.5) + self.beta\n",
    "        return gln_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PackedDeformableConvolution1d(DeformableConvolution1d):\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 kernel_size: int,\n",
    "                 stride: int = 1,\n",
    "                 padding: int | Literal['valid', 'same'] = 'valid',\n",
    "                 dilation: int = 1,\n",
    "                 groups: int = 1,\n",
    "                 bias: bool = True,\n",
    "                 padding_mode: str = 'reflect',\n",
    "                 offset_groups: int = 1,\n",
    "                 device: str = 'cpu',\n",
    "                 interpolation_func: Callable = linear_interpolation,\n",
    "                 unconstrained: str = None,\n",
    "                 *args, **kwargs) -> None:\n",
    "        \n",
    "        assert offset_groups in [1, in_channels], 'offset groups only implemented for 1 or in_channels'\n",
    "        \n",
    "        super().__init__(in_channels,\n",
    "                         out_channels,\n",
    "                         kernel_size,\n",
    "                         stride,\n",
    "                         padding,\n",
    "                         dilation,\n",
    "                         groups,\n",
    "                         bias,\n",
    "                         padding_mode,\n",
    "                         device,\n",
    "                         interpolation_func,\n",
    "                         unconstrained,\n",
    "                         *args, **kwargs)\n",
    "        \n",
    "        self.offset_groups = offset_groups\n",
    "\n",
    "        self.offset_dconv = nn.Conv1d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=in_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=1,\n",
    "            groups=in_channels,\n",
    "            padding=padding,\n",
    "            padding_mode=padding_mode,\n",
    "            bias=False\n",
    "        )\n",
    "        self.offset_dconv_norm = GlobalLayerNormalization(\n",
    "            in_channels\n",
    "        )\n",
    "        self.offset_dconv_prelu = nn.LeakyReLU()\n",
    "        \n",
    "        \n",
    "        self.offset_pconv = nn.Conv1d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=kernel_size*offset_groups,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            bias=False\n",
    "        )\n",
    "        self.offset_pconv_norm = GlobalLayerNormalization(\n",
    "            kernel_size * offset_groups\n",
    "        )\n",
    "        self.offset_pconv_prelu = nn.LeakyReLU()\n",
    "\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, with_offsets: bool = False) -> torch.Tensor:\n",
    "        # offsets = self.offset_dconv(x)\n",
    "        # offsets = self.offset_dconv_norm(self.offset_dconv_prelu(offsets).moveaxis(1, 2)).moveaxis(2, 1)\n",
    "        \n",
    "        self.device = x.device\n",
    "        \n",
    "        assert str(x.device) == str(self.device), 'x and the deformable conv must be on same device'\n",
    "        # assert str(x.device) == str(offsets.device), 'x and offsets must be on same device'\n",
    "        \n",
    "        offsets = self.offset_pconv(x)\n",
    "        # offsets = self.offset_pconv_norm(\n",
    "        #     self.offset_pconv_prelu(offsets).moveaxis(1, 2)\n",
    "        # ).moveaxis(2, 1)\n",
    "        offsets = offsets.unsqueeze(0).chunk(self.offset_groups, dim=2)\n",
    "        offsets = torch.vstack(offsets).moveaxis((0, 2), (1, 3))\n",
    "        \n",
    "        if with_offsets:\n",
    "            return super().forward(x, offsets), offsets\n",
    "        else:\n",
    "            return super().forward(x, offsets)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers using Deformable Convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aeon.datasets import load_arrow_head, load_osuleaf, load_acsf1, load_classification\n",
    "import numpy as np\n",
    "\n",
    "X_train, y_train, _ = load_classification(name='CinCECGTorso', split='TRAIN')\n",
    "y_train = y_train.astype(float)\n",
    "\n",
    "X_test, y_test, _ = load_classification(name='CinCECGTorso', split='TEST')\n",
    "y_test = y_test.astype(float)\n",
    "\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(np.unique(y_train))\n",
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.unique(y_train)[0] > 0.:\n",
    "    y_train = y_train - 1\n",
    "    y_test = y_test - 1\n",
    "    \n",
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_dataset = TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).long())\n",
    "test_dataset = TensorDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test).long())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from pytorch_lightning.utilities.types import STEP_OUTPUT, OptimizerLRScheduler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class FCN(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, num_classes: int) -> None:\n",
    "        super().__init__()\n",
    "        self.in_channels = 1\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.layers = nn.Sequential(*[\n",
    "            nn.Conv1d(in_channels=self.in_channels, out_channels=128, kernel_size=8, stride=1, padding='same'),\n",
    "            nn.BatchNorm1d(num_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=128, out_channels=256, kernel_size=5, stride=1, padding='same'),\n",
    "            nn.BatchNorm1d(num_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=256, out_channels=128, kernel_size=3, stride=1, padding='same'),\n",
    "            nn.BatchNorm1d(num_features=128),\n",
    "            nn.ReLU(),\n",
    "        ])\n",
    "\n",
    "        self.linear = nn.Linear(128, self.num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        self.criteria = nn.CrossEntropyLoss()\n",
    "\n",
    "    def configure_optimizers(self) -> OptimizerLRScheduler:\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return self.softmax(self.linear(x.mean(dim=-1)))\n",
    "    \n",
    "    def training_step(self, batch, batch_idx) -> torch.Tensor:\n",
    "        x, y = batch\n",
    "        \n",
    "        y_pred = self(x)\n",
    "        \n",
    "        loss = self.criteria(y_pred, y)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        \n",
    "        acc = accuracy_score(y_pred.argmax(dim=-1).cpu().numpy(), y.cpu().numpy())\n",
    "        self.log('train_acc', acc, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx) -> Any:\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        \n",
    "        acc = accuracy_score(y_pred.argmax(dim=-1).cpu().numpy(), y.cpu().numpy())\n",
    "        self.log('test_acc', acc, prog_bar=True)\n",
    "        \n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "for experiment_id in range(10):\n",
    "    fcn = FCN(num_classes=num_classes)\n",
    "    fcn_trainer = pl.Trainer(max_epochs=300, accelerator='gpu', devices=-1)\n",
    "    fcn_trainer.fit(fcn, train_loader)\n",
    "    \n",
    "    fcn = fcn.to(device)\n",
    "    fcn.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_preds = []\n",
    "        y_s = []\n",
    "        \n",
    "        for x, y in test_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "        \n",
    "            y_pred = fcn(x)\n",
    "            y_pred = y_pred.argmax(dim=-1).cpu().tolist()\n",
    "            \n",
    "            y_preds.extend(y_pred)\n",
    "\n",
    "            y_s.extend(y.cpu().numpy())\n",
    "\n",
    "        accuracies.append(accuracy_score(y_preds, y_s))\n",
    "\n",
    "    print(np.mean(accuracies))\n",
    "    print(np.std(accuracies))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(accuracies), np.std(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.utilities.types import OptimizerLRScheduler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DefFCN(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, num_classes) -> None:\n",
    "        super().__init__()\n",
    "        self.in_channels = 1\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(*[\n",
    "            PackedDeformableConvolution1d(\n",
    "                in_channels=self.in_channels, out_channels=128, kernel_size=8, padding='same', stride=1\n",
    "            ),\n",
    "            nn.BatchNorm1d(num_features=128),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            # nn.Conv1d(in_channels=self.in_channels, out_channels=128, kernel_size=8, stride=1, padding='same'),\n",
    "            # nn.BatchNorm1d(num_features=128),\n",
    "            # nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv1d(in_channels=128, out_channels=256, kernel_size=5, stride=1, padding='same'),\n",
    "            nn.BatchNorm1d(num_features=256),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv1d(in_channels=256, out_channels=128, kernel_size=3, stride=1, padding='same'),\n",
    "            nn.BatchNorm1d(num_features=128),\n",
    "            nn.LeakyReLU(),\n",
    "        ])\n",
    "        \n",
    "        self.criteria = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.linear = nn.Linear(128, self.num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def _set_lr(self, module, grad_in, grad_out):\n",
    "        new_grad = []\n",
    "        for i in range(len(grad_in)):\n",
    "            if grad_in[i] is not None:\n",
    "                new_grad.append(grad_in[i] * self.lr_ratio)\n",
    "            else:\n",
    "                new_grad.append(grad_in[i])\n",
    "                \n",
    "        new_grad = tuple(new_grad)\n",
    "        return new_grad\n",
    "        \n",
    "    def configure_optimizers(self) -> OptimizerLRScheduler:\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_blocks(x)\n",
    "        \n",
    "        return self.softmax(self.linear(x.mean(dim=-1)))\n",
    "    \n",
    "    def training_step(self, batch, batch_idx) -> torch.Tensor:\n",
    "        x, y = batch\n",
    "        \n",
    "        y_pred = self(x)\n",
    "        \n",
    "        loss = self.criteria(y_pred, y)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        \n",
    "        acc = accuracy_score(y_pred.argmax(dim=-1).cpu().numpy(), y.cpu().numpy())\n",
    "        self.log('train_acc', acc, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx) -> Any:\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        \n",
    "        acc = accuracy_score(y_pred.argmax(dim=-1).cpu().numpy(), y.cpu().numpy())\n",
    "        self.log('test_acc', acc, prog_bar=True)\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "for experiment_id in range(10):\n",
    "    def_fcn = DefFCN(num_classes=num_classes)\n",
    "    def_fcn_trainer = pl.Trainer(max_epochs=300, accelerator='gpu', devices=-1)\n",
    "    def_fcn_trainer.fit(def_fcn, train_loader)\n",
    "    \n",
    "    def_fcn = fcn.to(device)\n",
    "    def_fcn.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_preds = []\n",
    "        y_s = []\n",
    "        \n",
    "        for x, y in test_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "        \n",
    "            y_pred = def_fcn(x)\n",
    "            y_pred = y_pred.argmax(dim=-1).cpu().tolist()\n",
    "            \n",
    "            y_preds.extend(y_pred)\n",
    "\n",
    "            y_s.extend(y.cpu().numpy())\n",
    "\n",
    "        accuracies.append(accuracy_score(y_preds, y_s))\n",
    "\n",
    "    print(np.mean(accuracies))\n",
    "    print(np.std(accuracies))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Complex Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
